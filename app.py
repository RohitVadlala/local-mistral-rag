import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import requests

# âœ… Load FAISS index and document chunks
index = faiss.read_index("faiss_index.bin")
with open("chunks.txt", "r", encoding="utf-8") as f:
    chunks = f.read().split("\n---\n")

# âœ… Load embedding model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# âœ… Retrieve top-k relevant chunks
def retrieve_context(question, top_k=3):
    q_embedding = model.encode([question])
    distances, indices = index.search(q_embedding, top_k)
    return [chunks[i] for i in indices[0]]

# âœ… Build prompt from context
def generate_prompt(question, context_chunks):
    context = "\n\n".join(context_chunks)
    return f"""You are an AI assistant. Answer the following question based on the context below.

Context:
{context}

Question: {question}

Answer:"""

# âœ… Send prompt to Ollama-hosted Mistral model
def call_mistral(prompt):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "mistral",
        "prompt": prompt,
        "stream": False
    }

    try:
        print("ğŸ“¤ Sending prompt to Ollama...")
        response = requests.post(url, json=payload, timeout=300)
        response.raise_for_status()
        result = response.json()
        return result["response"]
    except requests.exceptions.RequestException as e:
        return f"âŒ Request failed: {e}"
    except KeyError:
        return "âŒ Invalid response format."

# âœ… Streamlit UI
st.set_page_config(page_title="ğŸ“„ Local PDF Q&A with Mistral", layout="centered")
st.title("ğŸ“„ PDF Q&A Assistant (powered by Local Mistral)")

question = st.text_input("ğŸ’¬ Ask your question about the PDF content")

if question:
    with st.spinner("ğŸ” Retrieving relevant context and generating answer..."):
        context_chunks = retrieve_context(question)
        prompt = generate_prompt(question, context_chunks)
        answer = call_mistral(prompt)

    st.markdown("### ğŸ¤– Mistral's Answer")
    st.success(answer)
